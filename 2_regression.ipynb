{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd16064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyfixest.estimation import fepois\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "PRE_START = '2021-01-01'\n",
    "EXP_END = '2025-09-01'\n",
    "REF_PERIOD = '2022-10-01'\n",
    "\n",
    "#List seniority DWA folder\n",
    "FILE_PATH = \"./data/seniority_DWA_data_CLEAN/\"\n",
    "files_seniority_DWA = os.listdir(FILE_PATH)\n",
    "files_seniority_DWA.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74450ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_panel(df):\n",
    "    print(\"--- Starting Panel Balancing ---\")\n",
    "    \n",
    "    # 1. Ensure Date Format\n",
    "    df = df.copy()\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "    \n",
    "    # 2. Define the Full Time Range \n",
    "    # This creates a list of 57 months\n",
    "    all_months = pd.date_range(start=PRE_START, end=EXP_END, freq='MS')\n",
    "    \n",
    "    # 3. Identify Unique Entity Pairs (Firm + DWA + Seniority)\n",
    "    # We include seniority so the new '0' rows inherit the correct seniority label\n",
    "    unique_keys = df[['firm_id', 'dwa_id', 'seniority']].drop_duplicates()\n",
    "    \n",
    "    print(f\"Unique Firm-Task pairs to balance: {len(unique_keys):,}\")\n",
    "    \n",
    "    # 4. Create the 'Skeleton' (The Target Grid)\n",
    "    # We use a cross-join to repeat every unique pair for every month\n",
    "    # This results in: (Num_Pairs * 57) rows\n",
    "    \n",
    "    # Create a temporary key for the cross join\n",
    "    unique_keys['_temp_key'] = 1\n",
    "    months_df = pd.DataFrame({'month': all_months, '_temp_key': 1})\n",
    "    \n",
    "    # Cross join: This creates the perfect empty grid\n",
    "    skeleton = pd.merge(unique_keys, months_df, on='_temp_key').drop('_temp_key', axis=1)\n",
    "    \n",
    "    print(f\"Target balanced size: {len(skeleton):,} rows\")\n",
    "    \n",
    "    # 5. Merge the Original Data onto the Skeleton\n",
    "    # Use LEFT JOIN: Keep the skeleton, fill data where it exists\n",
    "    balanced_df = pd.merge(\n",
    "        skeleton, \n",
    "        df, \n",
    "        on=['firm_id', 'dwa_id', 'seniority', 'month'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Fill Missing FTEs with 0\n",
    "    # Any row that existed in skeleton but not in df is now NaN. We make it 0.\n",
    "    balanced_df['FTE'] = balanced_df['FTE'].fillna(0)\n",
    "    \n",
    "    # 7. Recalculate 'time_j' (required for the regression later)\n",
    "    # Reference Date: Oct 2022\n",
    "    ref_dt = pd.to_datetime('2022-10-01')\n",
    "    balanced_df['time_j'] = (balanced_df['month'].dt.year - ref_dt.year) * 12 + \\\n",
    "                            (balanced_df['month'].dt.month - ref_dt.month)\n",
    "    \n",
    "    print(\"--- Balancing Complete ---\")\n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7f0df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created results directory: ./results\n",
      "Processing file: seniority_1_data.parquet\n",
      "--- Starting Panel Balancing ---\n",
      "Unique Firm-Task pairs to balance: 1,645,523\n",
      "Target balanced size: 93,794,811 rows\n",
      "--- Balancing Complete ---\n",
      "----------------------------------------\n",
      "Final Dataset Size: 93,794,811 rows\n",
      "Final Unique Companies: 3,922\n",
      "Final Unique Tasks: 954\n",
      "Starting regression loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 954/954 [07:59<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Extraction Complete. Rows: 54378\n",
      "Saving results to Parquet: ./results/regression_results_seniority_1_data.parquet\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Processing file: seniority_2_data.parquet\n",
      "--- Starting Panel Balancing ---\n",
      "Unique Firm-Task pairs to balance: 1,710,958\n",
      "Target balanced size: 97,524,606 rows\n",
      "--- Balancing Complete ---\n",
      "----------------------------------------\n",
      "Final Dataset Size: 97,524,606 rows\n",
      "Final Unique Companies: 3,920\n",
      "Final Unique Tasks: 995\n",
      "Starting regression loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995/995 [08:26<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Extraction Complete. Rows: 56715\n",
      "Saving results to Parquet: ./results/regression_results_seniority_2_data.parquet\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Processing file: seniority_3_data.parquet\n",
      "--- Starting Panel Balancing ---\n",
      "Unique Firm-Task pairs to balance: 1,304,275\n",
      "Target balanced size: 74,343,675 rows\n",
      "--- Balancing Complete ---\n",
      "----------------------------------------\n",
      "Final Dataset Size: 74,343,675 rows\n",
      "Final Unique Companies: 3,841\n",
      "Final Unique Tasks: 863\n",
      "Starting regression loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 863/863 [07:07<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Extraction Complete. Rows: 49191\n",
      "Saving results to Parquet: ./results/regression_results_seniority_3_data.parquet\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Processing file: seniority_4_data.parquet\n",
      "--- Starting Panel Balancing ---\n",
      "Unique Firm-Task pairs to balance: 1,230,921\n",
      "Target balanced size: 70,162,497 rows\n",
      "--- Balancing Complete ---\n",
      "----------------------------------------\n",
      "Final Dataset Size: 70,162,497 rows\n",
      "Final Unique Companies: 3,827\n",
      "Final Unique Tasks: 807\n",
      "Starting regression loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807/807 [05:47<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Extraction Complete. Rows: 45999\n",
      "Saving results to Parquet: ./results/regression_results_seniority_4_data.parquet\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Processing file: seniority_5_6_7_data.parquet\n",
      "--- Starting Panel Balancing ---\n",
      "Unique Firm-Task pairs to balance: 1,203,695\n",
      "Target balanced size: 68,610,615 rows\n",
      "--- Balancing Complete ---\n",
      "----------------------------------------\n",
      "Final Dataset Size: 68,610,615 rows\n",
      "Final Unique Companies: 3,908\n",
      "Final Unique Tasks: 683\n",
      "Starting regression loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 683/683 [05:33<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Extraction Complete. Rows: 38931\n",
      "Saving results to Parquet: ./results/regression_results_seniority_5_6_7_data.parquet\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyfixest.estimation import fepois\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the output directory path\n",
    "RESULTS_DIR = \"./results\"\n",
    "\n",
    "# --- CHECK AND CREATE RESULTS FOLDER ---\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "    print(f\"Created results directory: {RESULTS_DIR}\")\n",
    "# ---------------------------------------\n",
    "\n",
    "PRE_START = '2021-01-01'\n",
    "EXP_END = '2025-09-01'\n",
    "REF_PERIOD = '2022-10-01'\n",
    "\n",
    "#List seniority DWA folder\n",
    "FILE_PATH = \"./data/seniority_DWA_data_CLEAN/\"\n",
    "files_seniority_DWA = os.listdir(FILE_PATH)\n",
    "files_seniority_DWA.sort()\n",
    "\n",
    "# ... (The balance_panel function definition goes here) ...\n",
    "\n",
    "# %%\n",
    "for file_path in files_seniority_DWA:\n",
    "\n",
    "    company_DWA_df = pd.read_parquet(os.path.join(FILE_PATH, file_path))\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # APPLY THE BALANCING\n",
    "    # ==========================================\n",
    "\n",
    "    company_DWA_df = balance_panel(company_DWA_df)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Final Dataset Size: {len(company_DWA_df):,} rows\")\n",
    "    print(f\"Final Unique Companies: {company_DWA_df['firm_id'].nunique():,}\")\n",
    "    print(f\"Final Unique Tasks: {company_DWA_df['dwa_id'].nunique():,}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # REGRESSION PER TASK\n",
    "    # ==========================================\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. SETUP\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # Test on first 5 DWAs\n",
    "    target_dwas = company_DWA_df['dwa_id'].unique()\n",
    "    df_subset = company_DWA_df[company_DWA_df['dwa_id'].isin(target_dwas)].copy()\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_subset['month'] = pd.to_datetime(df_subset['month'])\n",
    "    df_subset['month_str'] = df_subset['month'].dt.strftime('%Y-%m-%d')\n",
    "    df_subset['H_total'] = df_subset.groupby(['firm_id', 'seniority', 'month'])['FTE'].transform('sum')\n",
    "    df_subset['Z_control'] = np.log(df_subset['H_total'] - df_subset['FTE'] + 1)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. ROBUST EXTRACTION LOOP\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # This Regex looks for \"[T.\" followed by a date \"2021-01-01\" followed by \"]\"\n",
    "    # It ignores the \"base='2022-10-01'\" part of the string.\n",
    "    date_pattern = re.compile(r\"\\[T\\.(\\d{4}-\\d{2}-\\d{2})\\]\")\n",
    "\n",
    "    results_list = []\n",
    "    errors = []\n",
    "\n",
    "    print(\"Starting regression loop...\")\n",
    "    grouped_iterator = df_subset.groupby(['dwa_id', 'seniority'])\n",
    "\n",
    "    for (dwa_id, seniority_level), subset in tqdm(grouped_iterator):\n",
    "        try:\n",
    "            # Data Checks\n",
    "            if subset['FTE'].sum() == 0 or subset['firm_id'].nunique() < 2:\n",
    "                continue\n",
    "\n",
    "            # Run PPML\n",
    "            model = fepois(\n",
    "                fml = f\"FTE ~ Z_control + i(month_str, ref='{REF_PERIOD}') | firm_id\",\n",
    "                data = subset,\n",
    "                vcov = \"hetero\"\n",
    "            )\n",
    "\n",
    "            # Extract\n",
    "            coefs = model.coef()\n",
    "            se = model.se()\n",
    "            \n",
    "            # Iterate through coefficients\n",
    "            for name, beta_value in coefs.items():\n",
    "                \n",
    "                # Use Regex to capture the specific time dummy date\n",
    "                match = date_pattern.search(name)\n",
    "                \n",
    "                if match:\n",
    "                    # match.group(1) is the date string inside the brackets (e.g., '2021-01-01')\n",
    "                    extracted_date = match.group(1)\n",
    "                    std_err = se.get(name, np.nan)\n",
    "                    \n",
    "                    results_list.append({\n",
    "                        'dwa_id': dwa_id,\n",
    "                        'seniority': seniority_level,\n",
    "                        'month': extracted_date,\n",
    "                        'beta': beta_value,\n",
    "                        'se': std_err\n",
    "                    })\n",
    "            \n",
    "            # CRITICAL: Manually add the Reference Period (Beta = 0)\n",
    "            # This ensures your plots don't have a gap at Oct 2022\n",
    "            results_list.append({\n",
    "                'dwa_id': dwa_id,\n",
    "                'seniority': seniority_level,\n",
    "                'month': REF_PERIOD,\n",
    "                'beta': 0.0,\n",
    "                'se': 0.0\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append((dwa_id, seniority_level, str(e)))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. SAVE & INSPECT\n",
    "    # ---------------------------------------------------------\n",
    "    ts_results = pd.DataFrame(results_list)\n",
    "\n",
    "    if not ts_results.empty:\n",
    "        # Convert month to datetime for sorting\n",
    "        ts_results['month'] = pd.to_datetime(ts_results['month'])\n",
    "        ts_results = ts_results.sort_values(['dwa_id', 'seniority', 'month'])\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Extraction Complete. Rows: {len(ts_results)}\")\n",
    "\n",
    "        # Save to Parquet, using the predefined RESULTS_DIR\n",
    "        output_path = os.path.join(RESULTS_DIR, f\"regression_results_{file_path}\")\n",
    "        print(f\"Saving results to Parquet: {output_path}\")\n",
    "        ts_results.to_parquet(output_path, index=False)\n",
    "\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results found. Check errors.\")\n",
    "        if errors:\n",
    "            print(errors[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfixest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
