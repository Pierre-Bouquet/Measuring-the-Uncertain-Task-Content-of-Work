{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d81075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4092 parquet files for processing.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============ Setup logging ============\n",
    "# Setup simple logging for demonstration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Path to the directory containing processed parquet files\n",
    "data_dir = Path(\"./data/company_data_processed\")\n",
    "parquet_files_processed_RAW = list(data_dir.glob(\"*.parquet\"))\n",
    "print(f\"Using {len(parquet_files_processed_RAW)} parquet files for processing.\")\n",
    "\n",
    "# Constants\n",
    "SENIORITY_GROUPS = [[1], [2], [3], [4], [5, 6, 7]]\n",
    "OUTPUT_DIR = Path(\"./data/seniority_DWA_data_RAW\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Required columns for consistency\n",
    "COLS = [\"firm_id\", \"dwa_id\", \"month\", \"seniority\", \"FTE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d68d7",
   "metadata": {},
   "source": [
    "# Merging datasets on Seniority Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e4dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting processing of 4092 files...\n",
      "INFO:__main__:Saving data for seniority level: 1\n",
      "INFO:__main__:Saving data for seniority level: 2\n",
      "INFO:__main__:Saving data for seniority level: 3\n",
      "INFO:__main__:Saving data for seniority level: 4\n",
      "INFO:__main__:Saving data for seniority level: 5_6_7\n"
     ]
    }
   ],
   "source": [
    "def process_seniority_data(file_list):\n",
    "    # 1. Initialize buckets to hold dataframes for each seniority group\n",
    "    buckets = {tuple(level): [] for level in SENIORITY_GROUPS}\n",
    "\n",
    "    logger.info(f\"Starting processing of {len(file_list)} files...\")\n",
    "\n",
    "    # 2. Iterate through files ONCE (I/O Heavy Step)\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            # Read the full file\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Distribute data to the appropriate bucket\n",
    "            for level in SENIORITY_GROUPS:\n",
    "                # Filter in memory\n",
    "                subset = df[df[\"seniority\"].isin(level)]\n",
    "                \n",
    "                if not subset.empty:\n",
    "                    buckets[tuple(level)].append(subset)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "    # 3. Concatenate and Save (CPU Heavy Step)\n",
    "    for level, frames in buckets.items():\n",
    "        group_name = '_'.join(map(str, level))\n",
    "        logger.info(f\"Saving data for seniority level: {group_name}\")\n",
    "\n",
    "        if frames:\n",
    "            full_df = pd.concat(frames, ignore_index=True)\n",
    "        else:\n",
    "            # Create empty DF with schema if no data found\n",
    "            full_df = pd.DataFrame(columns=COLS)\n",
    "\n",
    "        # === GROUP SENIORITY [5,6,7] into 5 \"Seniority\" ===\n",
    "        if level == (5, 6, 7):\n",
    "            # 1. Group by firm_id, dwa_id, month and sum FTE\n",
    "            company_DWA_df = full_df.groupby(['firm_id', 'dwa_id', 'month'])['FTE'].sum().reset_index()\n",
    "            # 2. Assign the new seniority value\n",
    "            company_DWA_df[\"seniority\"] = 5\n",
    "            \n",
    "            # Replace the original full_df with the aggregated one\n",
    "            full_df = company_DWA_df\n",
    "            \n",
    "            # Update COLS to match the new structure before dropna\n",
    "            # Note: 'seniority' is now the new, constant value, which aligns with the original 'COLS'\n",
    "        # === END MODIFICATION ===\n",
    "\n",
    "        # Drop NAs\n",
    "        full_df = full_df.dropna()\n",
    "\n",
    "        # Save to Parquet\n",
    "        output_path = OUTPUT_DIR / f\"seniority_{group_name}_data_RAW.parquet\"\n",
    "        full_df.to_parquet(\n",
    "            output_path, \n",
    "            index=False, \n",
    "            compression=\"snappy\"\n",
    "        )\n",
    "\n",
    "process_seniority_data(parquet_files_processed_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73c61b",
   "metadata": {},
   "source": [
    "# Filtering Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03356b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 parquet files for processing.\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing processed parquet files\n",
    "data_dir = Path(\"./data/seniority_DWA_data_RAW\")\n",
    "parquet_files_processed_RAW = list(data_dir.glob(\"*.parquet\"))\n",
    "print(f\"Using {len(parquet_files_processed_RAW)} parquet files for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870b9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0. CONFIGURATION\n",
    "# ---------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "OUTPUT_DIR = Path(\"./data/seniority_DWA_data_CLEAN\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Groups\n",
    "SENIORITY_GROUPS = [[1], [2], [3], [4], [5, 6, 7]]\n",
    "\n",
    "# Filter Parameters\n",
    "PRE_START = '2021-01-01'\n",
    "PRE_END = '2022-10-01'\n",
    "TAU_FIRMS = 100 # Minimum unique firms       \n",
    "TAU_MASS_PCT = 0.01 # Minimum mass share percentage\n",
    "\n",
    "# Map raw integer to group tuple key\n",
    "SENIORITY_MAP = {}\n",
    "for group in SENIORITY_GROUPS:\n",
    "    for raw_val in group:\n",
    "        SENIORITY_MAP[raw_val] = tuple(group)\n",
    "\n",
    "def get_stats_structure():\n",
    "    return {\n",
    "        tuple(group): {'fte_sums': {}, 'firm_sets': {}} \n",
    "        for group in SENIORITY_GROUPS\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. PASS 1: INDEPENDENT VALIDITY CHECK\n",
    "# ---------------------------------------------------------\n",
    "def get_valid_ids_per_group(file_list, tau_firms=TAU_FIRMS, tau_mass_pct=TAU_MASS_PCT):\n",
    "    logger.info(\"--- PASS 1: Calculating Independent Filters per Group ---\")\n",
    "    \n",
    "    group_stats = get_stats_structure()\n",
    "    \n",
    "    # A. ACCUMULATE STATS\n",
    "    for i, file_path in enumerate(file_list):\n",
    "        try:\n",
    "            # Load only needed columns\n",
    "            df = pd.read_parquet(file_path, columns=[\"firm_id\", \"dwa_id\", \"month\", \"seniority\", \"FTE\"])\n",
    "            df['month'] = pd.to_datetime(df['month'])\n",
    "            \n",
    "            # Filter Pre-treatment\n",
    "            df = df[(df['month'] >= PRE_START) & (df['month'] <= PRE_END)]\n",
    "            \n",
    "            if df.empty: continue\n",
    "\n",
    "            df['group_key'] = df['seniority'].map(SENIORITY_MAP)\n",
    "            \n",
    "            # Aggregate stats per group\n",
    "            for group_key, subset in df.groupby('group_key'):\n",
    "                if group_key not in group_stats: continue\n",
    "                \n",
    "                # FTE Sums\n",
    "                fte_counts = subset.groupby('dwa_id')['FTE'].sum()\n",
    "                for dwa, fte in fte_counts.items():\n",
    "                    group_stats[group_key]['fte_sums'][dwa] = group_stats[group_key]['fte_sums'].get(dwa, 0) + fte\n",
    "                \n",
    "                # Firm Counts (Set)\n",
    "                firm_groups = subset.groupby('dwa_id')['firm_id'].unique()\n",
    "                for dwa, firms in firm_groups.items():\n",
    "                    if dwa not in group_stats[group_key]['firm_sets']:\n",
    "                        group_stats[group_key]['firm_sets'][dwa] = set()\n",
    "                    group_stats[group_key]['firm_sets'][dwa].update(firms)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Pass 1: Skipping {file_path}: {e}\")\n",
    "\n",
    "    # B. CALCULATE VALID LISTS\n",
    "    valid_ids_dict = {}\n",
    "\n",
    "    for group_key in SENIORITY_GROUPS:\n",
    "        key = tuple(group_key)\n",
    "        stats = group_stats[key]\n",
    "        \n",
    "        total_group_fte = sum(stats['fte_sums'].values())\n",
    "        valid_set = set()\n",
    "        \n",
    "        # Check every task in this group\n",
    "        all_dwa_ids = set(stats['fte_sums'].keys())\n",
    "        \n",
    "        for dwa in all_dwa_ids:\n",
    "            task_fte = stats['fte_sums'][dwa]\n",
    "            unique_firms = len(stats['firm_sets'].get(dwa, set()))\n",
    "            \n",
    "            mass_share = (task_fte / total_group_fte * 100) if total_group_fte > 0 else 0\n",
    "            \n",
    "            # APPLY CRITERIA\n",
    "            if unique_firms >= tau_firms and mass_share >= tau_mass_pct:\n",
    "                valid_set.add(dwa)\n",
    "        \n",
    "        valid_ids_dict[key] = valid_set\n",
    "        logger.info(f\"Group {key}: {len(valid_set)} valid tasks identified (Share > {tau_mass_pct}% & Firms > {tau_firms})\")\n",
    "    return valid_ids_dict\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. PASS 2: GROUP-SPECIFIC FILTERING\n",
    "# ---------------------------------------------------------\n",
    "def process_with_group_masks(file_list, valid_ids_dict):\n",
    "    logger.info(\"--- PASS 2: Applying Group-Specific Masks & Saving ---\")\n",
    "    \n",
    "    # Buckets to hold the FINAL filtered dataframes\n",
    "    buckets = {tuple(level): [] for level in SENIORITY_GROUPS}\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            # Load Full Data\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Split by seniority FIRST, then filter using the specific mask\n",
    "            for level in SENIORITY_GROUPS:\n",
    "                key = tuple(level)\n",
    "                \n",
    "                # 1. Isolate the Seniority Level\n",
    "                subset = df[df[\"seniority\"].isin(level)]\n",
    "                \n",
    "                if subset.empty:\n",
    "                    continue\n",
    "\n",
    "                # 2. Retrieve the Valid IDs for THIS level\n",
    "                # If key missing (rare), default to empty set (drop all)\n",
    "                valid_mask = valid_ids_dict.get(key, set())\n",
    "                \n",
    "                # 3. Apply the specific filter\n",
    "                filtered_subset = subset[subset['dwa_id'].isin(valid_mask)]\n",
    "                \n",
    "                if not filtered_subset.empty:\n",
    "                    buckets[key].append(filtered_subset)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pass 2: Error processing {file_path}: {e}\")\n",
    "\n",
    "    # 3. SAVE RESULTS\n",
    "    for level, frames in buckets.items():\n",
    "        group_name = '_'.join(map(str, level))\n",
    "        \n",
    "        if frames:\n",
    "            full_df = pd.concat(frames, ignore_index=True)\n",
    "            full_df = full_df.dropna()\n",
    "            \n",
    "            out_path = OUTPUT_DIR / f\"seniority_{group_name}_data.parquet\"\n",
    "            full_df.to_parquet(out_path, index=False, compression=\"snappy\")\n",
    "            logger.info(f\"Saved: {out_path} (Rows: {len(full_df)})\")\n",
    "        else:\n",
    "            logger.warning(f\"No data saved for {group_name} (Bucket empty after filtering)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a527ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:--- PASS 1: Calculating Independent Filters per Group ---\n",
      "INFO:__main__:Group (1,): 954 valid tasks identified (Share > 0.01% & Firms > 100)\n",
      "INFO:__main__:Group (2,): 995 valid tasks identified (Share > 0.01% & Firms > 100)\n",
      "INFO:__main__:Group (3,): 863 valid tasks identified (Share > 0.01% & Firms > 100)\n",
      "INFO:__main__:Group (4,): 807 valid tasks identified (Share > 0.01% & Firms > 100)\n",
      "INFO:__main__:Group (5, 6, 7): 683 valid tasks identified (Share > 0.01% & Firms > 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "VALID TASKS PER SENIORITY LEVEL\n",
      "Criteria: >100 firms AND >0.01% mass (Pre-treatment)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# EXECUTION\n",
    "valid_map = get_valid_ids_per_group(parquet_files_processed_RAW)\n",
    "\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"VALID TASKS PER SENIORITY LEVEL\")\n",
    "print(f\"Criteria: >{TAU_FIRMS} firms AND >{TAU_MASS_PCT}% mass (Pre-treatment)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "total_unique_tasks = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ceaab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:--- PASS 2: Applying Group-Specific Masks & Saving ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seniority Level 1       :    954 valid tasks\n",
      "Seniority Level 2       :    995 valid tasks\n",
      "Seniority Level 3       :    863 valid tasks\n",
      "Seniority Level 4       :    807 valid tasks\n",
      "Seniority Level 5_6_7   :    683 valid tasks\n",
      "----------------------------------------\n",
      "Total Unique Tasks (Union)   : 1,244\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved: data/seniority_DWA_data_CLEAN/seniority_1_data.parquet (Rows: 77852578)\n",
      "INFO:__main__:Saved: data/seniority_DWA_data_CLEAN/seniority_2_data.parquet (Rows: 82441791)\n",
      "INFO:__main__:Saved: data/seniority_DWA_data_CLEAN/seniority_3_data.parquet (Rows: 59299324)\n",
      "INFO:__main__:Saved: data/seniority_DWA_data_CLEAN/seniority_4_data.parquet (Rows: 57444498)\n",
      "INFO:__main__:Saved: data/seniority_DWA_data_CLEAN/seniority_5_6_7_data.parquet (Rows: 58256159)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the map to print counts\n",
    "for group, valid_set in valid_map.items():\n",
    "    # Format the tuple key (e.g., (5, 6, 7) -> \"5_6_7\") for cleaner display\n",
    "    group_name = '_'.join(map(str, group))\n",
    "    \n",
    "    count = len(valid_set)\n",
    "    total_unique_tasks.update(valid_set)\n",
    "    \n",
    "    print(f\"Seniority Level {group_name:<7} : {count:>6,} valid tasks\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Unique Tasks (Union)   : {len(total_unique_tasks):,}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "process_with_group_masks(parquet_files_processed_RAW, valid_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfixest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
